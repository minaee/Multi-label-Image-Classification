{"cells":[{"cell_type":"markdown","metadata":{"id":"Ydg5f3hjDv7q"},"source":["# CS596 Finla Project Part 1A Introduction: Multi-label Image Classification"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40368,"status":"ok","timestamp":1649905796726,"user":{"displayName":"Mitra Rezaei","userId":"03281736141526679318"},"user_tz":240},"id":"Id6oeW7pDv7t","outputId":"00ce6ffb-8dac-4e83-8811-329764e8a30c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Collecting scipy==1.1.0\n","  Downloading scipy-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (31.2 MB)\n","\u001b[K     |████████████████████████████████| 31.2 MB 65.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.1.0) (1.21.5)\n","Installing collected packages: scipy\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.4.1\n","    Uninstalling scipy-1.4.1:\n","      Successfully uninstalled scipy-1.4.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pymc3 3.11.4 requires scipy>=1.2.0, but you have scipy 1.1.0 which is incompatible.\n","plotnine 0.6.0 requires scipy>=1.2.0, but you have scipy 1.1.0 which is incompatible.\n","jax 0.3.4 requires scipy>=1.2.1, but you have scipy 1.1.0 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed scipy-1.1.0\n","/content/drive/MyDrive/CS 596 - Final Project\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%pip install scipy==1.1.0\n","%cd \"drive/MyDrive/CS 596 - Final Project\"\n","\n","\n","import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torchvision\n","\n","from torchvision import transforms\n","from sklearn.metrics import average_precision_score\n","from PIL import Image, ImageDraw\n","import matplotlib.pyplot as plt\n","from kaggle_submission import output_submission_csv\n","from classifier import SimpleClassifier\n","from voc_dataloader import VocDataset, VOC_CLASSES\n","\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"-yY2_oYWESxZ"},"source":["## Upload Dataset PASCAL VOC 2007 dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":220992,"status":"ok","timestamp":1649084332786,"user":{"displayName":"Mitra Rezaei","userId":"03281736141526679318"},"user_tz":240},"id":"Lle7RV_WDxMT","outputId":"708c4d45-5ce0-41d9-9119-8508d8794223"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2022-04-04 14:55:11--  http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n","Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n","Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 460032000 (439M) [application/x-tar]\n","Saving to: ‘VOCtrainval_06-Nov-2007.tar’\n","\n","VOCtrainval_06-Nov- 100%[===================>] 438.72M  30.0MB/s    in 16s     \n","\n","2022-04-04 14:55:27 (27.8 MB/s) - ‘VOCtrainval_06-Nov-2007.tar’ saved [460032000/460032000]\n","\n","--2022-04-04 14:57:04--  http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n","Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n","Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 451020800 (430M) [application/x-tar]\n","Saving to: ‘VOCtest_06-Nov-2007.tar’\n","\n","VOCtest_06-Nov-2007 100%[===================>] 430.13M  30.9MB/s    in 16s     \n","\n","2022-04-04 14:57:20 (27.1 MB/s) - ‘VOCtest_06-Nov-2007.tar’ saved [451020800/451020800]\n","\n"]}],"source":["#!/usr/bin/env bash\n","\n","# download train\n","!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n","!tar -xf VOCtrainval_06-Nov-2007.tar\n","!mv VOCdevkit VOCdevkit_2007\n","\n","# download test and combine into same directory\n","!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n","!tar -xf VOCtest_06-Nov-2007.tar\n","!mv VOCdevkit/VOC2007 VOCdevkit_2007/VOC2007test\n","!rmdir VOCdevkit\n","\n","# MIRROR Links (comment out above and uncomment out below if host.robots.ox.au.uk is down)\n","\n","# wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n","# tar -xf VOCtrainval_06-Nov-2007.tar\n","# mv VOCdevkit VOCdevkit_2007\n","\n","# wget http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar\n","# tar -xf VOCtest_06-Nov-2007.tar\n","# mv VOCdevkit/VOC2007 VOCdevkit_2007/VOC2007test\n","# rmdir VOCdevkit"]},{"cell_type":"markdown","metadata":{"id":"h5ySeSz_Dv7u"},"source":["# Multi-label Classification\n","In this project, you train a classifier to do multi-label classificaton on the PASCAL VOC 2007 dataset. The dataset has 20 different class which can appear in any given image. Your classifier will predict whether each class appears in an image. This task is slightly different from exclusive multiclass classification like the ImageNet competition where only a single most appropriate class is predicted for an image.\n","\n","## Part 1A\n","You will use this notebook to warm up with pytorch and the code+dataset that we will use for the project. \n","\n","### What to do\n","In part 1A, You are asked to run below experiments. You don't need to change hyperparameters for this Part 1A's experiments. (the following code provides everything that you will need.)\n","1. to train a simple network (defined in ```classifiers.py```) \n","2. to train the AlexNet (PyTorch built-in) \n","    - from scratch \n","    - finetuning AlexNet pretrained on ImageNet\n","\n","\n","    \n","### What to submit\n","We ask you to run the following code and report the results in your project submission. You may want to leverage this part 1A get yourself familiar with PyTorch.\n","\n","You will the need the numbers and plots this notebook outputs for reports, but you are not required to submit this notebook as a printed pdf. "]},{"cell_type":"markdown","metadata":{"id":"iWenpkJ1Dv7v"},"source":["## Reading Pascal Data"]},{"cell_type":"markdown","metadata":{"id":"2r1-SONwDv7v"},"source":["### Loading Training Data"]},{"cell_type":"markdown","metadata":{"id":"Z2O9OUEhDv7v"},"source":["In the following cell we will load the training data and also apply some transforms to the data. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L3Ck6g-1Dv7w"},"outputs":[],"source":["# Transforms applied to the training data\n","normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                     std= [0.229, 0.224, 0.225])\n","\n","train_transform = transforms.Compose([\n","            transforms.Resize(227),\n","            transforms.CenterCrop(227),\n","            transforms.ToTensor(),\n","            normalize\n","        ])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1092442,"status":"ok","timestamp":1649762897445,"user":{"displayName":"Mitra Rezaei","userId":"03281736141526679318"},"user_tz":240},"id":"GeOfGeukDv7w","outputId":"167b5b05-eaa3-4797-cdd3-b6f626bc9d6e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/CS 596 - Final Project/voc_dataloader.py:137: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(box_indices),\n"]}],"source":["ds_train = VocDataset('VOCdevkit_2007/VOC2007/','train',train_transform)"]},{"cell_type":"markdown","metadata":{"id":"su1VEx3BDv7x"},"source":["### Loading Validation Data"]},{"cell_type":"markdown","metadata":{"id":"7LBG1JE-Dv7x"},"source":["We will load the test data for the PASCAL VOC 2007 dataset. Do __NOT__ add data augmentation transforms to validation data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CoYovmuyDv7x"},"outputs":[],"source":["# Transforms applied to the testing data\n","test_transform = transforms.Compose([\n","            transforms.Resize(227),\n","            transforms.CenterCrop(227),\n","            transforms.ToTensor(),\n","            normalize,\n","        ])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ch1fq4sNDv7y","executionInfo":{"status":"ok","timestamp":1649763961511,"user_tz":240,"elapsed":1064075,"user":{"displayName":"Mitra Rezaei","userId":"03281736141526679318"}},"outputId":"344b2611-edd1-4035-e557-540289ff5f65"},"outputs":[{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/CS 596 - Final Project/voc_dataloader.py:137: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(box_indices),\n"]}],"source":["ds_val = VocDataset('VOCdevkit_2007/VOC2007/','val',test_transform)"]},{"cell_type":"markdown","metadata":{"id":"Z3HA0WHkDv7y"},"source":["### Visualizing the Data\n","\n","PASCAL VOC has bounding box annotations in addition to class labels. Use the following code to visualize some random examples and corresponding annotations from the train set. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UMtJsUxrDv7y"},"outputs":[],"source":["for i in range(5):\n","    idx = np.random.randint(0, len(ds_train.names)+1)\n","    _imgpath = os.path.join('VOCdevkit_2007/VOC2007/', 'JPEGImages', ds_train.names[idx]+'.jpg')\n","    img = Image.open(_imgpath).convert('RGB')\n","    draw = ImageDraw.Draw(img)\n","    for j in range(len(ds_train.box_indices[idx])):\n","        obj = ds_train.box_indices[idx][j]\n","        draw.rectangle(list(obj), outline=(255,0,0))\n","        draw.text(list(obj[0:2]), ds_train.classes[ds_train.label_order[idx][j]], fill=(0,255,0))\n","    plt.figure(figsize = (10,10))\n","    plt.imshow(np.array(img))"]},{"cell_type":"markdown","metadata":{"id":"Ghyk-EgNDv7z"},"source":["# Classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"86NZPWyQDv7z"},"outputs":[],"source":["# declare what device to use: gpu/cpu\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kT9XK8ebDv7z"},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(dataset=ds_train,\n","                                               batch_size=50, \n","                                               shuffle=True,\n","                                               num_workers=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JpPhSZ2QDv7z"},"outputs":[],"source":["val_loader = torch.utils.data.DataLoader(dataset=ds_val,\n","                                               batch_size=50, \n","                                               shuffle=True,\n","                                               num_workers=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"chKELC0iDv70"},"outputs":[],"source":["def train_classifier(train_loader, classifier, criterion, optimizer):\n","    classifier.train()\n","    loss_ = 0.0\n","    losses = []\n","    for i, (images, labels) in enumerate(train_loader):\n","        images, labels = images.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        logits = classifier(images)\n","        loss = criterion(logits, labels)\n","        loss.backward()\n","        optimizer.step()\n","        losses.append(loss)\n","    return torch.stack(losses).mean().item()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bQFwDpTeDv70"},"outputs":[],"source":["def test_classifier(test_loader, classifier, criterion, print_ind_classes=True, print_total=True):\n","    classifier.eval()\n","    losses = []\n","    with torch.no_grad():\n","        y_true = np.zeros((0,21))\n","        y_score = np.zeros((0,21))\n","        for i, (images, labels) in enumerate(test_loader):\n","            images, labels = images.to(device), labels.to(device)\n","            logits = classifier(images)\n","            y_true = np.concatenate((y_true, labels.cpu().numpy()), axis=0)\n","            y_score = np.concatenate((y_score, logits.cpu().numpy()), axis=0)\n","            loss = criterion(logits, labels)\n","            losses.append(loss.item())\n","        aps = []\n","        # ignore first class which is background\n","        for i in range(1, y_true.shape[1]):\n","            ap = average_precision_score(y_true[:, i], y_score[:, i])\n","            if print_ind_classes:\n","                print('-------  Class: {:<12}     AP: {:>8.4f}  -------'.format(VOC_CLASSES[i], ap))\n","            aps.append(ap)\n","        \n","        mAP = np.mean(aps)\n","        test_loss = np.mean(losses)\n","        if print_total:\n","            print('mAP: {0:.4f}'.format(mAP))\n","            print('Avg loss: {}'.format(test_loss))\n","        \n","    return mAP, test_loss, aps"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oL-litMmDv70"},"outputs":[],"source":["# plot functions\n","def plot_losses(train, val, test_frequency, num_epochs):\n","    plt.plot(train, label=\"train\")\n","    indices = [i for i in range(num_epochs) if ((i+1)%test_frequency == 0 or i ==0)]\n","    plt.plot(indices, val, label=\"val\")\n","    plt.title(\"Loss Plot\")\n","    plt.ylabel(\"Loss\")\n","    plt.xlabel(\"Epoch\")\n","    plt.legend()\n","    plt.show()\n","    \n","def plot_mAP(train, val, test_frequency, num_epochs):\n","    indices = [i for i in range(num_epochs) if ((i+1)%test_frequency == 0 or i ==0)]\n","    plt.plot(indices, train, label=\"train\")\n","    plt.plot(indices, val, label=\"val\")\n","    plt.title(\"mAP Plot\")\n","    plt.ylabel(\"mAP\")\n","    plt.xlabel(\"Epoch\")\n","    plt.legend()\n","    plt.show()\n","    "]},{"cell_type":"markdown","metadata":{"id":"hrYA4ZqUDv70"},"source":["## Training the network \n","\n","The simple network you are given as is will allow you to reach around 0.15-0.2 mAP. In this project, you will find ways to design a better network. Save plots and final test mAP scores as you will be adding these to the writeup."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KThW3vtCDv70"},"outputs":[],"source":["def train(classifier, num_epochs, train_loader, val_loader, criterion, optimizer, test_frequency=5):\n","    train_losses = []\n","    train_mAPs = []\n","    val_losses = []\n","    val_mAPs = []\n","\n","    for epoch in range(1,num_epochs+1):\n","        print(\"Starting epoch number \" + str(epoch))\n","        train_loss = train_classifier(train_loader, classifier, criterion, optimizer)\n","        train_losses.append(train_loss)\n","        print(\"Loss for Training on Epoch \" +str(epoch) + \" is \"+ str(train_loss))\n","        if(epoch%test_frequency==0 or epoch==1):\n","            mAP_train, _, _ = test_classifier(train_loader, classifier, criterion, False, False)\n","            train_mAPs.append(mAP_train)\n","            mAP_val, val_loss, _ = test_classifier(val_loader, classifier, criterion)\n","            print('Evaluating classifier')\n","            print(\"Mean Precision Score for Testing on Epoch \" +str(epoch) + \" is \"+ str(mAP_val))\n","            val_losses.append(val_loss)\n","            val_mAPs.append(mAP_val)\n","    \n","    return classifier, train_losses, val_losses, train_mAPs, val_mAPs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UkFt0SlbDv71"},"outputs":[],"source":["classifier = SimpleClassifier().to(device)\n","# You can use this function to reload a network you have already saved previously\n","#classifier.load_state_dict(torch.load('voc_classifier.pth'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sFiBx2hzDv71","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649764272193,"user_tz":240,"elapsed":313,"user":{"displayName":"Mitra Rezaei","userId":"03281736141526679318"}},"outputId":"e01523ec-839e-4d8c-f54c-ef682223fd34"},"outputs":[{"output_type":"stream","name":"stdout","text":["MultiLabelSoftMarginLoss()\n"]}],"source":["criterion = nn.MultiLabelSoftMarginLoss()\n","optimizer = torch.optim.SGD(classifier.parameters(), lr=0.01, momentum=0.9)\n","print(criterion)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eqrOcAEJDv71","scrolled":true},"outputs":[],"source":["# Training the Classifier\n","num_epochs = 20\n","test_frequency = 5\n","\n","classifier, train_losses, val_losses, train_mAPs, val_mAPs = train(classifier, num_epochs, train_loader, val_loader, criterion, optimizer, test_frequency)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zog-dD45Dv71"},"outputs":[],"source":["# Compare train and validation metrics\n","plot_losses(train_losses, val_losses, test_frequency, num_epochs)\n","plot_mAP(train_mAPs, val_mAPs, test_frequency, num_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HUJv45XWDv71"},"outputs":[],"source":["# Save the clssifier network\n","# Suggestion: you can save checkpoints of your network during training and reload them later\n","torch.save(classifier.state_dict(), './voc_simple_classifier.pth')"]},{"cell_type":"markdown","metadata":{"id":"2ZM-JcwcDv71"},"source":["# Evaluate on test set\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YWUgmRszDv71"},"outputs":[],"source":["ds_test = VocDataset('VOCdevkit_2007/VOC2007test/','test', test_transform)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=ds_test,\n","                                               batch_size=50, \n","                                               shuffle=False,\n","                                               num_workers=1)\n","\n","# Transforms applied to the testing data\n","test_transform = transforms.Compose([\n","            transforms.Resize(227),\n","            transforms.CenterCrop(227),\n","            transforms.ToTensor(),\n","            normalize,\n","        ])\n","\n","mAP_test, test_loss, test_aps = test_classifier(test_loader, classifier, criterion)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-dYQGgiwDv71"},"outputs":[],"source":["output_submission_csv('my_solution.csv', test_aps)"]},{"cell_type":"markdown","metadata":{"id":"I7xV0g_ODv72"},"source":["# AlexNet Baselines (From Scratch)"]},{"cell_type":"markdown","metadata":{"id":"KqKZvbsdDv72"},"source":["AlexNet was one of the earliest deep learning models to have success in classification. In this section we will be running classification with AlexNet as a baseline. Furthermore, we will run an ImageNet-pretrained AlexNet to observe the impact of well-trained features. Save plots and final test mAP scores as you will be adding these to the writeup."]},{"cell_type":"markdown","metadata":{"id":"NaGl2wg8Dv72"},"source":["## Running AlexNet\n","\n","In this section, we train AlexNet from scratch using the same hyperparameters as our previous experiment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_IYXDSfBDv72","scrolled":true},"outputs":[],"source":["num_epochs = 20\n","test_frequency = 5\n","\n","# Change classifier to AlexNet\n","classifier = torchvision.models.alexnet(pretrained=False)\n","classifier.classifier._modules['6'] = nn.Linear(4096, 21)   \n","classifier = classifier.to(device)\n","\n","criterion = nn.MultiLabelSoftMarginLoss()\n","\n","optimizer = torch.optim.SGD(classifier.parameters(), lr=0.01, momentum=0.9)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9m930YxDv72"},"outputs":[],"source":["classifier, train_losses, val_losses, train_mAPs, val_mAPs = train(classifier, num_epochs, train_loader, val_loader, criterion, optimizer, test_frequency)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fn6z3tLzDv72"},"outputs":[],"source":["plot_losses(train_losses, val_losses, test_frequency, num_epochs)\n","plot_mAP(train_mAPs, val_mAPs, test_frequency, num_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vp7MQLPDDv72"},"outputs":[],"source":["mAP_test, test_loss, test_aps = test_classifier(test_loader, classifier, criterion)\n","print(\"Test mAP: \", mAP_test)"]},{"cell_type":"markdown","metadata":{"id":"cfAkArpGDv72"},"source":["You should notice somewhat poor performance. You could try running AlexNet with an Adam optimizer instead with learning rate 1e-4 to see if that makes a difference. This experiment is not required for the writeup, but it may show you the importance of a good learning rate and optimizer."]},{"cell_type":"markdown","metadata":{"id":"V6VFKSzMDv72"},"source":["## Pretrained AlexNet"]},{"cell_type":"markdown","metadata":{"id":"3kvfk4GzDv72"},"source":["Here we look at the impact of pretrained features. This model's weights were trained on ImageNet, which is a much larger dataset. How do pretrained features perform on VOC? Why do you think there is such a large difference in performance?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fK08xBAcDv73"},"outputs":[],"source":["num_epochs = 20\n","test_frequency = 5\n","\n","# Load Pretrained AlexNet\n","classifier = torchvision.models.alexnet(pretrained=True)\n","classifier.classifier._modules['6'] = nn.Linear(4096, 21)   \n","classifier = classifier.to(device)\n","optimizer = torch.optim.SGD(classifier.parameters(), lr=0.01, momentum=0.9)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hmr0l3RmDv73"},"outputs":[],"source":["classifier, train_losses, val_losses, train_mAPs, val_mAPs = train(classifier, num_epochs, train_loader, val_loader, criterion, optimizer, test_frequency)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RjqjuGTRDv73"},"outputs":[],"source":["plot_losses(train_losses, val_losses, test_frequency, num_epochs)\n","plot_mAP(train_mAPs, val_mAPs, test_frequency, num_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oGdrObAIDv73"},"outputs":[],"source":["mAP_test, test_loss, test_aps = test_classifier(test_loader, classifier, criterion)\n","print(\"Test mAP: \", mAP_test)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"MP3_P1A_Introduction.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}